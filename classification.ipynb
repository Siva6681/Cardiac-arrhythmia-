{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.11.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/HP 840 G6/Downloads/CardiacArythimia-20250208T053644Z-001/CardiacArythimia/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV , cross_validate, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier , GradientBoostingClassifier , AdaBoostClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score ,f1_score, confusion_matrix , classification_report, recall_score, precision_score\n",
    "\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('arrhythmia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[ 'Normal', \n",
    "'Ischemic changes (Coronary Artery Disease)',\n",
    "'Old Anterior Myocardial Infarction' ,\n",
    "'Old Inferior Myocardial Infarction' ,\n",
    "'Sinus tachycardy' ,\n",
    "'Sinus bradycardy' ,\n",
    "'Ventricular Premature Contraction (PVC)' ,\n",
    "'Supraventricular Premature Contraction',\n",
    "'Left bundle branch block' ,\n",
    " 'Right bundle branch block',\n",
    " '1. degree AtrioVentricular block' ,\n",
    " '2. degree AV block' ,\n",
    " '3. degree AV block',\n",
    "'Left ventricule hypertrophy' ,\n",
    "'Atrial Fibrillation or Flutter' ,\n",
    "'Others'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().sum():\n",
    "        a.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    nan_count = data.loc[:,i].isna().sum()\n",
    "    print(f\"Column {i} has {nan_count/X.shape[0]*100} % NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J column have  83% nan value hence we are dropping this column \n",
    "# and for rest we will use simple imputer for imputing the values \n",
    "X_new = X.drop('J',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['T','P','QRST','heartrate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in column:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Mean and median line plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Distribution KDE plot\n",
    "    sns.kdeplot(X_new[col], shade=True)\n",
    "    \n",
    "    # Vertical lines for mean and median\n",
    "    mean_val = X_new[col].mean()\n",
    "    median_val = X_new[col].median()\n",
    "    plt.axvline(x=mean_val, color='skyblue', linestyle='--', label='Mean')\n",
    "    plt.axvline(x=median_val, color='orange', linestyle='--', label='Median')\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Distribution of {col} with Mean and Median')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = Si.fit_transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t= pd.DataFrame(X_transform , columns=X_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in column:\n",
    "    print(X_t[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy = { 2: 100, 3: 100, 4: 100,5:100,6:100,7:100,8:100,9:100,10:100,14:100,15:100,16:100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming X and y are your feature and target variables\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ,X_test , y_train ,y_test = train_test_split(X_resampled,y_resampled,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(pipeline,name):\n",
    "    # Fit the pipeline\n",
    "   \n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "    metrics[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall , 'f1':f1}\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=pipeline.named_steps['classifier'].classes_,\n",
    "                yticklabels=pipeline.named_steps['classifier'].classes_)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics , pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms on Balance Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf1 = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "logistic_metrics, logistic_model = train_models(logistic_clf1, \"Logistic Regression\")\n",
    "filename = 'logistc_model.pkl'\n",
    "pickle.dump(logistic_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "\n",
    "svc_metrics , svc_model=train_models(svc_clf , \"svc_model\")\n",
    "\n",
    "filename1 = 'svc_model.pkl'\n",
    "pickle.dump(svc_clf, open(filename1, 'wb')) \n",
    "  \n",
    "# load the model \n",
    "load_model = pickle.load(open(filename1, 'rb')) \n",
    "  \n",
    "y_pred = load_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clf =Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "randmon_metrics , random_model = train_models(random_clf , \"Random_Forest\")\n",
    "filename = 'random_model.pkl'\n",
    "pickle.dump(random_model, open(filename, 'wb')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn =Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "knnmetrics , knn_model = train_models(knn , \"Knn\")\n",
    "filename = 'knn.pkl'\n",
    "pickle.dump(knn_model, open(filename, 'wb')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms on Imbalance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,X_test1,y_train1,y_test1=train_test_split(X_t,y,test_size=0.2) # when data is imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models1(pipeline,name):\n",
    "    # Fit the pipeline\n",
    "  \n",
    "\n",
    "    pipeline.fit(X_train1, y_train1)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test1)\n",
    "    accuracy = accuracy_score(y_test1, y_pred)\n",
    "    precision = precision_score(y_test1, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test1, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test1,y_pred,average='weighted')\n",
    "\n",
    "    metrics1[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall , 'f1':f1}\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test1, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test1, y_pred)\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=pipeline.named_steps['classifier'].classes_,\n",
    "                yticklabels=pipeline.named_steps['classifier'].classes_)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics , pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf2 = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "logistic_metrics1, logistic_model = train_models1(logistic_clf2, \"Logistic Regression\")\n",
    "filename = 'logistc_model1.pkl'\n",
    "pickle.dump(logistic_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf1 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "\n",
    "svc_metrics1 , svc_model1=train_models1(svc_clf1 , \"svc_model\")\n",
    "\n",
    "filename1 = 'svc_model1.pkl'\n",
    "pickle.dump(svc_model1, open(filename1, 'wb')) \n",
    "  \n",
    "# load the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clf1 =Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "randmon_metrics1 , random_model1 = train_models1(random_clf , \"Random_Forest\")\n",
    "filename = 'random_model1.pkl'\n",
    "pickle.dump(random_model1, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 =Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "knnmetrics1 , knn_model1 = train_models1(knn , \"Knn\")\n",
    "filename = 'knn1.pkl'\n",
    "pickle.dump(knn_model1, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of algorithms When Data is balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = list(metrics.keys())\n",
    "accuracy_values = [metrics[algo]['Accuracy'] for algo in algos]\n",
    "precision_values = [metrics[algo]['Precision'] for algo in algos]\n",
    "recall_values = [metrics[algo]['Recall'] for algo in algos]\n",
    "f1_values = [metrics[algo]['f1'] for algo in algos]\n",
    "\n",
    "# Plotting the metrics as line graphs\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "ax[0, 0].plot(algos, accuracy_values, marker='o', color='b', label='Accuracy')\n",
    "ax[0, 0].set_title('Accuracy')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "ax[0, 1].plot(algos, precision_values, marker='o', color='g', label='Precision')\n",
    "ax[0, 1].set_title('Precision')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[1, 0].plot(algos, recall_values, marker='o', color='r', label='Recall')\n",
    "ax[1, 0].set_title('Recall')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].plot(algos, f1_values, marker='o', color='orange', label='F1 Score')\n",
    "ax[1, 1].set_title('F1 Score')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of algorithms When Data is Imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = list(metrics1.keys())\n",
    "accuracy_values = [metrics1[algo]['Accuracy'] for algo in algos]\n",
    "precision_values = [metrics1[algo]['Precision'] for algo in algos]\n",
    "recall_values = [metrics1[algo]['Recall'] for algo in algos]\n",
    "f1_values = [metrics1[algo]['f1'] for algo in algos]\n",
    "\n",
    "# Plotting the metrics as line graphs\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "ax[0, 0].plot(algos, accuracy_values, marker='o', color='b', label='Accuracy')\n",
    "ax[0, 0].set_title('Accuracy')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "ax[0, 1].plot(algos, precision_values, marker='o', color='g', label='Precision')\n",
    "ax[0, 1].set_title('Precision')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[1, 0].plot(algos, recall_values, marker='o', color='r', label='Recall')\n",
    "ax[1, 0].set_title('Recall')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].plot(algos, f1_values, marker='o', color='orange', label='F1 Score')\n",
    "ax[1, 1].set_title('F1 Score')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "In conclusion, Random Forest and Logistic Regression demonstrate robust performance across imbalanced datasets, outperforming other algorithms even after dataset balancing. Their effectiveness suggests their suitability for handling imbalanced data scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYBRID MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best two model are random forest and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "hybrid_model = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='soft')\n",
    "\n",
    "filename = 'hybrid.pkl'\n",
    "pickle.dump(hybrid_model, open(filename, 'wb')) \n",
    "hybrid_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_pred = hybrid_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Hybrid Model Accuracy:\", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "# metrics1[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall , 'f1':f1}\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=hybrid_model.classes_,\n",
    "            yticklabels=hybrid_model.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train1)\n",
    "X_test_scaled = scaler.transform(X_test1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "hybrid_model = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='soft')\n",
    "\n",
    "filename = 'hybrid1.pkl'\n",
    "pickle.dump(hybrid_model, open(filename, 'wb')) \n",
    "hybrid_model.fit(X_train_scaled, y_train1)\n",
    "\n",
    "\n",
    "y_pred = hybrid_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "print(\"Hybrid Model Accuracy:\", accuracy)\n",
    "\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "precision1 = precision_score(y_test1, y_pred, average='weighted')\n",
    "recall1 = recall_score(y_test1, y_pred, average='weighted')\n",
    "f11 = f1_score(y_test1,y_pred,average='weighted')\n",
    "\n",
    "# metrics1[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall , 'f1':f1}\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test1, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test1, y_pred)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=hybrid_model.classes_,\n",
    "            yticklabels=hybrid_model.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see Hybrid Model is working good on balanced dataset instead of Inbalanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the hybrid model on best 7 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "hybrid_model = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# SelectKBest to select the best 10 features\n",
    "selector = SelectKBest(score_func=f_classif, k=7)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = X_train.columns[selector.get_support()]\n",
    "\n",
    "# Train the model on the selected features\n",
    "hybrid_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = hybrid_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print selected feature names\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=hybrid_model.classes_,\n",
    "            yticklabels=hybrid_model.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'scaler.pkl'\n",
    "pickle.dump(scaler, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'hybrid_features.pkl'\n",
    "pickle.dump(hybrid_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision , recall ,f1 ,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = [metrics[algo]['Accuracy'] for algo in algos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = [\"Logistic Regression \",\"SVM\" ,\"Random Forest\",\"KNN\" ,\"Hybrid Model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_values = [metrics[algo]['Precision'] for algo in algos]\n",
    "recall_values = [metrics[algo]['Recall'] for algo in algos]\n",
    "f1_values = [metrics[algo]['f1'] for algo in algos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_vals = [0.9748822605965463,\n",
    " 0.9748822605965463,\n",
    " 0.9921507064364207,\n",
    " 0.9434850863422292, 0.989010989010989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_vals = [0.9775669609794883,\n",
    " 0.9754963215614182,\n",
    " 0.9925072069644185,\n",
    " 0.9404076722784118 ,0.9896211231745446] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_vals=[0.9748822605965463,\n",
    " 0.9748822605965463,\n",
    " 0.9921507064364207,\n",
    " 0.9434850863422292, 0.989010989010989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_vals = [0.9734627547590529,\n",
    " 0.9745478173570239,\n",
    " 0.9920554087809106,\n",
    " 0.9376609456335065 , 0.9887950346737808]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#COMPARISON of ALL MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR BALANCED DATASET  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(al, acc_vals, color='skyblue')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Algorithms')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(al, rec_vals, color='green')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall of Different Algorithms')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(al, f1_vals, color='orange')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 score of Different Algorithms')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(al, prec_vals, color='brown')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision of Different Algorithms')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
